# IQID-Alphas Pipeline Development Plan

**Date**: June 19, 2025 (Revised)  
**Objective**: Phase 2 Implementation - Build production-ready pipelines using completed Phase 1 modules  
**Status**: ‚úÖ Phase 1 Complete ‚Üí üöÄ Phase 2 Implementation

---

## Executive Summary

This document outlines the implementation plan for Phase 2: Pipeline Integration & CLI Development. With Phase 1's modular core system completed, we now focus on building three production-ready pipelines that leverage these components through unified CLI interfaces.

### Target Pipelines
1. **Segmentation Pipeline**: Raw ‚Üí Segmented validation using `TissueSeparator`
2. **Alignment Pipeline**: Segmented ‚Üí Aligned validation using modular alignment package
3. **Coregistration Pipeline**: iQID+H&E multi-modal registration using `MultiModalAligner`

### Architecture Pattern
- **Thin Interface Design**: CLI scripts in `pipelines/` delegate to `iqid_alphas/pipelines/`
- **Configuration-Driven**: JSON-based parameter management
- **Unified CLI**: Single entry point with subcommands
- **Modular Integration**: Leverages completed Phase 1 components
- **Size-Constrained Modules**: No module exceeds 500 lines; split complex functionality into focused packages

---

## ‚úÖ Phase 1 Completed Infrastructure

### Available Modular Components

**Location**: `iqid_alphas/core/` ‚úÖ **COMPLETED**

#### A. **Tissue Segmentation** (`tissue_segmentation.py`)
- `TissueSeparator` class with adaptive clustering methods
- Integration with legacy process_object.py functionality
- Ground truth validation capabilities

```python
class TissueSeparator:
    def __init__(self, method='adaptive_clustering')
    def separate_tissues(self, raw_image_path: str) -> Dict[str, List[Path]]
    def validate_against_ground_truth(self, automated_results: Dict, ground_truth_dirs: Dict) -> Dict
```

#### B. **Alignment Package** (`alignment/`) ‚úÖ **MODULAR PACKAGE**
- `SimpleAligner`, `EnhancedAligner`, `StackAligner`, `UnifiedAligner` classes
- Bidirectional alignment methods with reference slice detection
- Noise-robust alignment (mask-based, template-based)

```python
from iqid_alphas.core.alignment import UnifiedAligner
aligner = UnifiedAligner(method='bidirectional')
results = aligner.align_slice_stack(slice_paths)
```

#### C. **Coregistration Package** (`coregistration/`) ‚úÖ **MODULAR PACKAGE**
- `MultiModalAligner` class for iQID+H&E registration
- Mutual information and correlation-based methods
- Quality assessment without ground truth

```python
from iqid_alphas.core.coregistration import MultiModalAligner
aligner = MultiModalAligner(method='mutual_information')
results = aligner.register_iqid_he_pair(iqid_dir, he_dir)
```

#### D. **Validation Package** (`validation/`) ‚úÖ **MODULAR PACKAGE**
- `ValidationSuite` orchestrator with specialized validators
- Comprehensive metrics (SSIM, IoU, Dice, MI)
- Statistical analysis and reporting

```python
from iqid_alphas.core.validation import ValidationSuite
validator = ValidationSuite()
report = validator.validate_pipeline_results(results)
```

#### E. **Batch Processing Package** (`batch_processing/`) ‚úÖ **MODULAR PACKAGE**
- `BatchProcessor` with parallel processing capabilities
- SQLite-based results storage
- Job scheduling and resource management

```python
from iqid_alphas.core.batch_processing import BatchProcessor
processor = BatchProcessor(data_path, output_dir)
results = processor.run_batch_job(samples, pipeline_func)
```

#### F. **Reverse Mapping Framework** (`mapping.py`) ‚úÖ **COMPLETED**
- Ground truth evaluation for segmentation and alignment
- Template matching and feature-based registration
- 100% accuracy validation on test data

```python
from iqid_alphas.core.mapping import ReverseMapper
mapper = ReverseMapper()
crop_result = mapper.find_crop_location(raw_image, segmented_slice)
transform_result = mapper.calculate_alignment_transform(original, aligned)
```

### Utility Modules ‚úÖ **COMPLETED**
- `iqid_alphas/utils/io_utils.py`: File I/O and image loading
- `iqid_alphas/utils/math_utils.py`: Mathematical operations and transforms

---

## Phase 2: Pipeline Integration & CLI Development ‚úÖ **CURRENT PHASE**

**Status**: üöÄ **IN PROGRESS** (Following phase2_plan_tracking.md)  
**Objective**: Build production-ready pipelines that leverage the modular core components developed in Phase 1

### **Task 2.1: Pipeline Infrastructure Development** 

**Objective**: Create the foundational pipeline framework that orchestrates core modules.

#### **Sub-Task 2.1.1: Create Pipeline Base Classes**
- **Action**: Implement `iqid_alphas/pipelines/base.py` with abstract pipeline interface
- **Deliverable**: `BasePipeline` class with common functionality (logging, config, progress tracking)
- **Dependencies**: Phase 1 core modules
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.1.2: Implement Configuration System**
- **Action**: Create `iqid_alphas/configs/` directory with JSON configuration templates
- **Deliverable**: Configuration loader and validator classes
- **Files**: 
  - `iqid_alphas/configs/base_config.json`
  - `iqid_alphas/configs/coregistration_config.json`
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.1.3: Create Progress Tracking System**
- **Action**: Implement progress reporting with logging integration
- **Deliverable**: `ProgressTracker` class with console and file output
- **Features**: Real-time progress bars, ETA calculation, performance metrics
- **Status**: ‚è≥ **PENDING**

### **Task 2.2: Segmentation Pipeline Implementation**

**Objective**: Create the Raw ‚Üí Segmented validation pipeline using Phase 1 tissue segmentation components.

#### **Sub-Task 2.2.1: Core Pipeline Implementation**
- **Action**: Create `iqid_alphas/pipelines/segmentation.py`
- **Deliverable**: `SegmentationPipeline` class
- **Key Features**:
  - Integration with `TissueSeparator` from Phase 1
  - Quality metric calculation (IoU, Dice, precision, recall)
- **Dependencies**: Phase 1 tissue segmentation, validation, batch processing modules
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.2.2: CLI Interface Creation**
- **Action**: Create `pipelines/segmentation.py` thin interface script
- **Deliverable**: Command-line tool with proper argument parsing
- **Features**: Input/output directory specification, progress reporting
- **Status**: ‚è≥ **PENDING**

### **Task 2.3: Alignment Pipeline Implementation**

**Objective**: Create the Segmented ‚Üí Aligned validation pipeline using Phase 1 alignment components.

#### **Sub-Task 2.3.1: Core Pipeline Implementation**
- **Action**: Create `iqid_alphas/pipelines/alignment.py`
- **Deliverable**: `AlignmentPipeline` class
- **Key Features**:
  - Integration with modular alignment package from Phase 1
  - Advanced quality metrics (SSIM, mutual information)
- **Dependencies**: Phase 1 alignment package, validation system
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.3.2: CLI Interface Creation**  
- **Action**: Create `pipelines/alignment.py` thin interface script
- **Deliverable**: Command-line tool for alignment validation
- **Features**: Similar CLI structure to segmentation with alignment-specific options
- **Status**: ‚è≥ **PENDING**

### **Task 2.4: Coregistration Pipeline Implementation**

**Objective**: Create the iQID+H&E co-registration pipeline using Phase 1 coregistration components.

#### **Sub-Task 2.4.1: Core Pipeline Implementation**
- **Action**: Create `iqid_alphas/pipelines/coregistration.py`
- **Deliverable**: `CoregistrationPipeline` class  
- **Key Features**:
  - Integration with `MultiModalAligner` from Phase 1
  - Proxy quality assessment (no ground truth available)
- **Dependencies**: Phase 1 coregistration package, batch processing
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.4.2: CLI Interface Creation**
- **Action**: Create `pipelines/coregistration.py` thin interface script  
- **Deliverable**: Command-line tool for coregistration
- **Features**: Multi-modal specific CLI options and validation
- **Status**: ‚è≥ **PENDING**

### **Task 2.5: Unified CLI System**

**Objective**: Create a single entry point CLI that orchestrates all pipelines.

#### **Sub-Task 2.5.1: Main CLI Implementation**
- **Action**: Create `iqid_alphas/cli.py` and update `iqid_alphas/__main__.py`
- **Deliverable**: Unified CLI with subcommands
- **Features**:
  - `python -m iqid_alphas segmentation <args>`
  - Common options: --config, --verbose, --output-dir
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.5.2: Help System and Documentation**
- **Action**: Implement comprehensive help system
- **Deliverable**: Rich help text, examples, and error messages
- **Features**: Context-sensitive help, example commands, troubleshooting guides
- **Status**: ‚è≥ **PENDING**

### **Task 2.6: Performance & Production Optimization**

**Objective**: Optimize pipelines for production use with large datasets.

#### **Sub-Task 2.6.1: Parallel Processing Implementation**
- **Action**: Add parallel processing capabilities to all pipelines
- **Deliverable**: Multi-threaded/multi-process execution
- **Features**: Configurable worker count, memory management, progress aggregation
- **Dependencies**: Phase 1 batch processing framework
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.6.2: Memory Optimization**
- **Action**: Implement memory-efficient processing for large datasets
- **Deliverable**: Streaming data processing, garbage collection optimization
- **Features**: Lazy loading, memory monitoring, automatic cleanup
- **Status**: ‚è≥ **PENDING**

### **Task 2.7: Integration Testing & Validation**

**Objective**: Ensure all components work together correctly with real data.

#### **Sub-Task 2.7.1: End-to-End Integration Tests**
- **Action**: Create comprehensive integration test suite
- **Deliverable**: `iqid_alphas/tests/integration/` test modules
- **Coverage**: Full pipeline execution, CLI testing, configuration validation
- **Status**: ‚è≥ **PENDING**

#### **Sub-Task 2.7.2: Real Data Validation**
- **Action**: Validate pipelines against known datasets
- **Deliverable**: Validation reports and benchmark results
- **Features**: Performance benchmarks, accuracy validation, regression testing
- **Status**: ‚è≥ **PENDING**

---

## Phase 3: Pipeline Interface & CLI Examples

### 3.1 Pipeline 1: Raw ‚Üí Segmented

**Interface**: `pipelines/segmentation.py`

```python
#!/usr/bin/env python3
"""
ReUpload Raw ‚Üí Segmented Pipeline

Validates automated tissue separation against ground truth.
Processes multi-tissue raw iQID images and separates them into individual tissue stacks.
"""

from iqid_alphas.pipelines import SegmentationPipeline

def main():
    parser = argparse.ArgumentParser(description="Raw to Segmented Validation Pipeline")
    parser.add_argument("--data", required=True, help="Path to ReUpload dataset")
    parser.add_argument("--output", default="outputs/raw_to_segmented", help="Output directory")
    parser.add_argument("--max-samples", type=int, help="Maximum samples to process")
    parser.add_argument("--config", help="Configuration file path")
    
    args = parser.parse_args()
    
    # Initialize and run pipeline
    pipeline = SegmentationPipeline(
        data_path=args.data,
        output_dir=args.output,
        config_path=args.config
    )
    
    # Execute validation
    results = pipeline.run_validation(max_samples=args.max_samples)
    
    # Generate comprehensive report
    pipeline.generate_report(results)
    
    # Print summary
    print_pipeline_summary(results)

def print_pipeline_summary(results: Dict):
    """Print pipeline execution summary"""
    print("\n" + "="*60)
    print("RAW ‚Üí SEGMENTED PIPELINE RESULTS")
    print("="*60)
    print(f"üìä Total sample groups processed: {results['total_groups']}")
    print(f"‚úÖ Successful segmentations: {results['successful_segmentations']}")
    print(f"‚ùå Failed segmentations: {results['failed_segmentations']}")
    print(f"üìà Average IoU score: {results['avg_iou']:.3f}")
    print(f"üìà Average Dice score: {results['avg_dice']:.3f}")
    print(f"‚è±Ô∏è  Total processing time: {results['total_time']:.2f}s")

if __name__ == "__main__":
    main()
```

**Core Implementation**: `iqid_alphas/pipelines/segmentation.py`

**Key Features**:
- Discovers raw image groups sharing the same base ID (D1M1, etc.)
- Processes each raw image to separate tissue types
- Validates against ground truth in `1_segmented/` directories
- Calculates comprehensive quality metrics (IoU, Dice, precision, recall)
- Generates visualizations comparing automated vs ground truth results

### 3.2 Pipeline 2: Segmented ‚Üí Aligned

**Interface**: `pipelines/alignment.py`

```python
#!/usr/bin/env python3
"""
ReUpload Segmented ‚Üí Aligned Pipeline

Validates automated slice alignment with advanced bidirectional methods.
Processes segmented tissue slices and aligns them into coherent 3D stacks.
"""

from iqid_alphas.pipelines import AlignmentPipeline

def main():
    parser = argparse.ArgumentParser(description="Segmented to Aligned Validation Pipeline")
    parser.add_argument("--data", required=True, help="Path to ReUpload dataset")
    parser.add_argument("--output", default="outputs/segmented_to_aligned", help="Output directory")
    parser.add_argument("--max-samples", type=int, help="Maximum samples to process")
    parser.add_argument("--alignment-method", default="bidirectional", 
                       choices=["bidirectional", "sequential", "reference_based"],
                       help="Alignment strategy")
    parser.add_argument("--reference-method", default="largest", 
                       choices=["largest", "central", "quality_based"],
                       help="Reference slice selection method")
    
    args = parser.parse_args()
    
    # Initialize pipeline with configuration
    pipeline = AlignmentPipeline(
        data_path=args.data,
        output_dir=args.output,
        alignment_method=args.alignment_method,
        reference_method=args.reference_method
    )
    
    # Execute validation
    results = pipeline.run_validation(max_samples=args.max_samples)
    
    # Generate comprehensive report
    pipeline.generate_report(results)
    
    # Print detailed summary
    print_alignment_summary(results)

def print_alignment_summary(results: Dict):
    """Print alignment pipeline execution summary"""
    print("\n" + "="*60)
    print("SEGMENTED ‚Üí ALIGNED PIPELINE RESULTS")
    print("="*60)
    print(f"üìä Total samples processed: {results['total_samples']}")
    print(f"‚úÖ Successful alignments: {results['successful_alignments']}")
    print(f"üìà Average SSIM score: {results['avg_ssim']:.3f}")
    print(f"üìà Average mutual information: {results['avg_mi']:.3f}")
    print(f"üéØ Average propagation quality: {results['avg_propagation_quality']:.3f}")
    
    print(f"\nüî¨ Method Performance:")
    for method, perf in results['method_performance'].items():
        print(f"   {method}: Success Rate {perf['success_rate']:.1%}, "
              f"Quality {perf['avg_quality']:.3f}")

if __name__ == "__main__":
    main()
```

**Core Implementation**: `iqid_alphas/pipelines/alignment.py`

**Key Features**:
- Implement bidirectional alignment methods
- Reference slice auto-detection (largest slice strategy)
- Bidirectional propagation (largest‚Üíup, largest‚Üídown)
- Noise-robust alignment (mask-based, template-based, adaptive)
- Validates against ground truth in `2_aligned/` directories
- Advanced quality metrics (SSIM, MI, propagation consistency)

### 3.3 Pipeline 3: iQID+H&E Co-registration

**Interface**: `pipelines/coregistration.py`

```python
#!/usr/bin/env python3
"""
DataPush1 iQID+H&E Co-registration Pipeline

Develops and evaluates multi-modal co-registration between iQID and H&E images.
No ground truth available - uses proxy metrics for quality assessment.
"""

from iqid_alphas.pipelines import CoregistrationPipeline

def main():
    parser = argparse.ArgumentParser(description="iQID+H&E Co-registration Pipeline")
    parser.add_argument("--data", required=True, help="Path to DataPush1 dataset")
    parser.add_argument("--output", default="outputs/iqid_he_coregistration", help="Output directory")
    parser.add_argument("--max-samples", type=int, help="Maximum sample pairs to process")
    parser.add_argument("--registration-method", default="mutual_information",
                       choices=["mutual_information", "feature_based", "intensity_based"],
                       help="Registration method")
    parser.add_argument("--pairing-strategy", default="automatic",
                       choices=["automatic", "manual", "fuzzy_matching"],
                       help="Sample pairing strategy")
    
    args = parser.parse_args()
    
    # Initialize pipeline
    pipeline = CoregistrationPipeline(
        data_path=args.data,
        output_dir=args.output,
        registration_method=args.registration_method,
        pairing_strategy=args.pairing_strategy
    )
    
    # Execute co-registration evaluation
    results = pipeline.run_evaluation(max_samples=args.max_samples)
    
    # Generate comprehensive report
    pipeline.generate_report(results)
    
    # Print summary
    print_coregistration_summary(results)

def print_coregistration_summary(results: Dict):
    """Print co-registration pipeline execution summary"""
    print("\n" + "="*60)
    print("iQID+H&E CO-REGISTRATION PIPELINE RESULTS")
    print("="*60)
    print(f"üìä Total sample pairs processed: {results['total_pairs']}")
    print(f"‚úÖ Successfully paired samples: {results['successful_pairings']}")
    print(f"üîó Successfully registered pairs: {results['successful_registrations']}")
    print(f"üìà Average mutual information: {results['avg_mutual_information']:.3f}")
    print(f"üìà Average structural similarity: {results['avg_structural_similarity']:.3f}")
    print(f"üéØ Average registration confidence: {results['avg_confidence']:.3f}")
    
    print(f"\nüß¨ Tissue Type Performance:")
    for tissue, perf in results['tissue_performance'].items():
        print(f"   {tissue}: Pairs {perf['total_pairs']}, "
              f"Success Rate {perf['success_rate']:.1%}")

if __name__ == "__main__":
    main()
```

**Core Implementation**: `iqid_alphas/pipelines/coregistration.py`

**Key Features**:
- Automated iQID-H&E sample pairing based on naming patterns
- Multi-modal registration using mutual information optimization
- Feature-based alignment with SIFT/ORB keypoints
- Qualitative assessment with proxy metrics (no ground truth)
- Comprehensive visualization of registration results

---

## Phase 4: Data Flow and Architecture

### 4.1 Overall Architecture

```
pipelines/          # Thin interface scripts
‚îú‚îÄ‚îÄ segmentation.py
‚îú‚îÄ‚îÄ alignment.py
‚îî‚îÄ‚îÄ coregistration.py

iqid_alphas/
‚îú‚îÄ‚îÄ core/                      # Core processing modules
‚îÇ   ‚îú‚îÄ‚îÄ tissue_segmentation.py
‚îÇ   ‚îú‚îÄ‚îÄ slice_alignment.py
‚îÇ   ‚îú‚îÄ‚îÄ coregistration.py
‚îÇ   ‚îú‚îÄ‚îÄ validation.py
‚îÇ   ‚îî‚îÄ‚îÄ batch_processor.py
‚îú‚îÄ‚îÄ pipelines/                 # Pipeline implementations
‚îÇ   ‚îú‚îÄ‚îÄ segmentation.py
‚îÇ   ‚îú‚îÄ‚îÄ alignment.py
‚îÇ   ‚îî‚îÄ‚îÄ coregistration.py
‚îú‚îÄ‚îÄ utils/                     # Utilities and adapters
‚îÇ   ‚îú‚îÄ‚îÄ legacy_adapters.py
‚îÇ   ‚îú‚îÄ‚îÄ transform_utils.py
‚îÇ   ‚îî‚îÄ‚îÄ io_utils.py
‚îî‚îÄ‚îÄ visualization/             # Enhanced visualization
    ‚îî‚îÄ‚îÄ enhanced_plotter.py
```

### 4.2 Data Flow Diagram

```
ReUpload Dataset
‚îú‚îÄ‚îÄ Raw Images ‚Üí [Pipeline 1] ‚Üí Segmented Validation
‚îú‚îÄ‚îÄ 1_segmented/ ‚Üí [Pipeline 2] ‚Üí Aligned Validation
‚îî‚îÄ‚îÄ 2_aligned/ (Ground Truth)

DataPush1 Dataset
‚îú‚îÄ‚îÄ iQID/ (Aligned) ‚Üí [Pipeline 3] ‚Üí Co-registration
‚îî‚îÄ‚îÄ H&E/ (Aligned) ‚Üí [Pipeline 3] ‚Üí Assessment
```

### 4.3 Configuration Management

**Location**: `iqid_alphas/configs/`

```
configs/
‚îú‚îÄ‚îÄ segmentation_config.json
‚îú‚îÄ‚îÄ alignment_config.json
‚îú‚îÄ‚îÄ coregistration_config.json
‚îî‚îÄ‚îÄ validation_config.json
```

**Example Configuration**: `segmentation_config.json`
```json
{
  "segmentation": {
    "method": "adaptive_clustering",
    "min_blob_area": 50,
    "max_blob_area": 50000,
    "preserve_blobs": true,
    "use_clustering": true
  },
  "validation": {
    "metrics": ["iou", "dice", "precision", "recall"],
    "iou_threshold": 0.5,
    "generate_visualizations": true
  },
  "processing": {
    "batch_size": 10,
    "memory_limit_gb": 8,
    "parallel_workers": 4
  },
  "output": {
    "save_intermediate": true,
    "compression": "lzw",
    "visualization_dpi": 300
  }
}
```

---

## Phase 5: Quality Assurance Framework

### 5.1 Testing Strategy

#### A. Unit Tests
**Location**: `iqid_alphas/tests/core/`
- Test each core module independently
- Mock data generation for consistent testing
- Performance benchmarking

#### B. Integration Tests
**Location**: `iqid_alphas/tests/pipelines/`
- End-to-end pipeline validation
- Data flow verification
- Error handling robustness

#### C. Validation Tests
**Location**: `iqid_alphas/tests/validation/`
- Ground truth comparison accuracy
- Metric calculation verification
- Statistical analysis validation

### 5.2 Performance Metrics

#### A. Processing Performance
- Execution time per sample
- Memory usage optimization
- Batch processing efficiency
- Parallel processing scaling

#### B. Quality Metrics
- **Segmentation**: IoU, Dice coefficient, precision, recall
- **Alignment**: SSIM, mutual information, propagation consistency
- **Co-registration**: Mutual information, structural similarity, confidence scores

#### C. Robustness Metrics
- Error rate under various conditions
- Noise tolerance assessment
- Edge case handling evaluation

### 5.3 Validation Framework

```python
# iqid_alphas/core/validation.py
class PipelineValidator:
    def __init__(self, pipeline_type: str)
    def validate_segmentation_pipeline(self, results: Dict) -> Dict
    def validate_alignment_pipeline(self, results: Dict) -> Dict
    def validate_coregistration_pipeline(self, results: Dict) -> Dict
    def generate_validation_report(self, validation_results: Dict) -> str
```

---

## Phase 6: Documentation and Reporting

### 6.1 User Documentation

#### A. Pipeline Usage Guides
**Location**: `docs/user_guides/pipelines/`
- `segmentation_guide.md`
- `alignment_guide.md`
- `coregistration_guide.md`

#### B. Configuration Documentation
**Location**: `docs/configuration/`
- Parameter descriptions and valid ranges
- Performance tuning guidelines
- Troubleshooting common issues

#### C. API Documentation
**Location**: `docs/api_reference/`
- Core module API reference
- Pipeline class documentation
- Integration examples

### 6.2 Automated Reporting

#### A. Pipeline Reports
- Processing summary statistics
- Quality metric distributions
- Failure analysis and recommendations
- Performance benchmarks

#### B. Visualization Dashboards
- Interactive result exploration
- Comparative analysis tools
- Quality trend monitoring

#### C. Scientific Validation
- Integration with dosimetry analysis (`old/iqid-alphas/iqid/dpk.py`)
- Consistency verification with published methods
- Reproducibility documentation

---

## Phase 7: Migration and Cleanup Strategy

### 7.1 Migration Timeline

#### Week 1-2: Core Infrastructure
- Implement core modules (`tissue_segmentation.py`, `slice_alignment.py`, `coregistration.py`)
- Create legacy adapters for `old/iqid-alphas/iqid/` integration
- Set up validation framework

#### Week 3-4: Pipeline Implementation
- Develop three pipeline classes
- Create thin interface scripts
- Implement configuration management

#### Week 5-6: Testing and Validation
- Comprehensive testing suite
- Performance optimization
- Documentation completion

#### Week 7: Cleanup and Migration
- Remove deprecated evaluation scripts
- Final validation against original results
- Production deployment preparation

### 7.2 File Removal Strategy

**Post-Implementation Cleanup**:
```bash
# Remove test evaluation scripts (after migration)
rm -rf evaluation/scripts/
rm -rf evaluation/utils/

# Keep only pipeline interfaces

pipelines/
‚îú‚îÄ‚îÄ segmentation.py
‚îú‚îÄ‚îÄ alignment.py
‚îî‚îÄ‚îÄ coregistration.py
```

### 7.3 Backward Compatibility is NOT Required

#### A. Legacy Support
- Do not maintain compatibility with existing CLI interfaces
- Do not provide migration utilities for old configurations
- Do not keep legacy code after refactoring

---

## Phase 8: Success Criteria and Deliverables

### 8.1 Technical Success Criteria

#### A. Functional Requirements
- ‚úÖ Three fully functional pipelines with thin interfaces
- ‚úÖ Core processing logic implemented in `iqid_alphas` package
- ‚úÖ Integration with legacy `old/iqid-alphas/iqid/` components
- ‚úÖ Comprehensive validation against ground truth data
- ‚úÖ Robust error handling and logging

#### B. Performance Requirements
- ‚úÖ Processing time comparable to or better than current scripts
- ‚úÖ Memory usage optimized for large datasets
- ‚úÖ Batch processing capabilities with progress tracking
- ‚úÖ Parallel processing support where applicable

#### C. Quality Requirements
- ‚úÖ Segmentation IoU > 0.7 against ground truth
- ‚úÖ Alignment SSIM > 0.8 against ground truth
- ‚úÖ Co-registration mutual information > baseline threshold
- ‚úÖ 95% success rate on validation datasets

### 8.2 Deliverables Checklist

#### A. Core Modules (6 files)
- [ ] `iqid_alphas/core/tissue_segmentation.py`
- [ ] `iqid_alphas/core/slice_alignment.py`
- [ ] `iqid_alphas/core/coregistration.py`
- [ ] `iqid_alphas/core/validation.py`
- [ ] `iqid_alphas/core/batch_processor.py`
- [ ] `iqid_alphas/utils/legacy_adapters.py`

#### B. Pipeline Classes (3 files)
- [ ] `iqid_alphas/pipelines/segmentation.py`
- [ ] `iqid_alphas/pipelines/alignment.py`
- [ ] `iqid_alphas/pipelines/coregistration.py`

#### C. Interface Scripts (3 files)
- [ ] `pipelines/segmentation.py`
- [ ] `pipelines/alignment.py`
- [ ] `pipelines/coregistration.py`

#### D. Configuration Files (4 files)
- [ ] `iqid_alphas/configs/segmentation_config.json`
- [ ] `iqid_alphas/configs/alignment_config.json`
- [ ] `iqid_alphas/configs/coregistration_config.json`
- [ ] `iqid_alphas/configs/validation_config.json`

#### E. Documentation (12 files)
- [ ] `docs/user_guides/pipelines/` (3 guides)
- [ ] `docs/api_reference/pipelines/` (3 API docs)
- [ ] `docs/configuration/` (3 config docs)
- [ ] `docs/PERFORMANCE_BENCHMARKS.md`
- [ ] `docs/VALIDATION_RESULTS.md`

#### F. Test Suite (15+ files)
- [ ] Unit tests for all core modules
- [ ] Integration tests for all pipelines
- [ ] Validation tests for all metrics
- [ ] Performance benchmarking tests

### 8.3 Quality Gates

#### A. Code Quality
- [ ] 100% test coverage for core modules
- [ ] Comprehensive error handling
- [ ] Performance profiling and optimization
- [ ] Code review and documentation

#### B. Validation Quality
- [ ] All pipelines pass validation against ground truth
- [ ] Performance metrics meet or exceed current implementation
- [ ] Robustness testing under various conditions
- [ ] Scientific validation with domain experts

#### C. Production Readiness
- [ ] Clean, maintainable architecture
- [ ] Comprehensive documentation
- [ ] User-friendly interfaces
- [ ] Automated testing and CI/CD integration

---

## Implementation Priority

### High Priority (Must Have)
1. **Core Infrastructure**: `tissue_segmentation.py`, `slice_alignment.py`, `coregistration.py`
2. **Pipeline 1**: Raw ‚Üí Segmented validation (most critical for validation)
3. **Pipeline 2**: Segmented ‚Üí Aligned validation (builds on Pipeline 1)
4. **Legacy Integration**: Proper utilization of `old/iqid-alphas/iqid/` components

### Medium Priority (Should Have)
1. **Pipeline 3**: iQID+H&E co-registration (new development)
2. **Advanced Validation**: Comprehensive quality metrics and reporting
3. **Performance Optimization**: Batch processing and parallel execution
4. **Configuration Management**: Flexible parameter tuning

### Low Priority (Nice to Have)
1. **Interactive Visualization**: Dashboard-style result exploration
2. **Advanced Analytics**: Statistical trend analysis and quality monitoring
3. **Cloud Integration**: Scalable processing infrastructure
4. **API Extensions**: REST API for remote processing

---

## Risk Assessment and Mitigation

### Technical Risks

#### A. Legacy Code Integration Complexity
**Risk**: Difficulty integrating `old/iqid-alphas/iqid/` components  
**Mitigation**: Create adapter pattern with thorough testing and validation

#### B. Performance Degradation
**Risk**: New implementation slower than current scripts  
**Mitigation**: Benchmark at each step, optimize critical paths, profile memory usage

#### C. Quality Regression
**Risk**: Lower accuracy compared to current evaluation methods  
**Mitigation**: Comprehensive validation against existing results, ground truth comparison

### Project Risks

#### A. Timeline Overrun
**Risk**: Development takes longer than planned  
**Mitigation**: Phased implementation, prioritize core functionality, parallel development

#### B. Resource Constraints
**Risk**: Insufficient computational resources for testing  
**Mitigation**: Optimize for memory efficiency, implement batch processing, use subset testing

#### C. Requirement Changes
**Risk**: Changing requirements during implementation  
**Mitigation**: Modular design, flexible configuration system, regular stakeholder review

---

## Conclusion

This comprehensive plan provides a roadmap for transforming the current evaluation test scripts into three robust, production-ready pipelines. The approach emphasizes:

1. **Clean Architecture**: Core logic in `iqid_alphas`, thin interfaces in `pipelines/`
2. **Legacy Integration**: Proper utilization of proven `old/iqid-alphas` methods
3. **Comprehensive Validation**: Rigorous testing against ground truth data
4. **Production Readiness**: Robust error handling, logging, and documentation
5. **Maintainability**: Modular design with clear separation of concerns

The implementation will proceed in phases, with continuous validation and testing to ensure quality and performance standards are met. Upon completion, the system will provide a solid foundation for iQID data processing evaluation and scientific analysis.

---

**Next Steps**: 
1. Review and approve this plan
2. Set up development environment and branch strategy
3. Begin Phase 1 implementation with core infrastructure
4. Establish continuous integration and testing framework
5. Schedule regular progress reviews and stakeholder updates
